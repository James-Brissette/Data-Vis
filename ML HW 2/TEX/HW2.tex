\documentclass[11pt,a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\setlength\abovedisplayskip{0pt}
\author{James Brissette}
\title{CS-6350: HW 2}
\begin{document}
	\maketitle
	
	\section{Linear Classifiers and Boolean Functions}
		\begin{enumerate}
			\item $\neg x_1 \vee x_2 \vee \neg x_3 $: Linearly separable $\rightarrow (1-x_1)+x_2 + (1-x_3) \geq 1$
			\item $(x_1 \vee x_2) \wedge x_3$: Linearly separable $\rightarrow \frac{1}{2}(x_1 + x_2) + x_3 \geq 1.5$
			\item $(x_1 \wedge \neg x_2) \vee \neg x_3$: Linearly separable $\rightarrow \frac{1}{2}(x_1 + (1-x_2))+(1-x_3) \geq 1$
			\item $x_1$ xor $x_2$ xor $x_3$: Not linearly separable.
			\item $x_1 \wedge \neg x_2 \wedge x_3$: Linearly separable $\rightarrow x_1 + (1-x_2) + x_3 \geq 3$
		\end{enumerate}
	
	\section{Feature transformations}
		\begin{enumerate}
			\item My function is $\phi ( x_{1} , x_{2} ) = ( x_{1}^{4} , x_{2}^{3} )$, which maps $x_{1}$ and $x_{2}$ to $x_{1}^{4}$ and $x_{2}^{3}$ respectively. And in this space the function is linearly separable as $$\phi(fr(x_1,x_2)) = \begin{cases}
			+1 \quad 17x_1-16x_2 \leq r \\
			-1 \quad otherwise
			\end{cases}$$
			\item We can see that since the original function $17x_1^4-16x_2^3$ was $\leq r$, it follows the linear classifier for the new function is equivalent to $-w^T\phi (x_1,x_2) \geq b$ where out weight vector $w$ is given by [17 -16] and $b=-r$
		\end{enumerate}
	
	\section{Mistake Bound Model of Learning}
		\begin{enumerate}
			\item $ $
			\begin{enumerate}
				\item Since the concept class C consists of one function for every combination of $z$, which is $\{0,1\}^n$, $\vert C_1 \vert = 2^n$ 
				\item The algorithm would proceed as follows:\\
				\textit{\textbf{Observe example \\
				predict 0\\
				if correct, no update, continue\\
				if incorrect, learning ends as the  target function has been learned.}}\\ \\
				Since there is only one target function, and given that the function only evaluates to 1 for one single instance of the sample space (when x = z), there will never be a mistake so long as the learner continues to predict 0. When the learner finally encounters $z$, it will update and can continue to progress with the assurance that since $z$ has been found, there will never be another mistake. 
			\end{enumerate}
			\item $ $
			\begin{enumerate}
				\item If we assume in the worst case that nature is adversarial, then the CON algorithm will make $n-1$ mistakes. Because there are $n$ function where $f_i(x) = x_i$. Suppose our target function is $f_n$ and out randomly drawn function was $f_1$, we would then have a mistake, and proceed to randomly choose from $C - \{f_1\}$. This would continue $n-1$ times until at last the last function left in $C$ to evaluate was the target function $f$.
				\item Since the halving algorithm predicts the majority label, $f_i$ would predict with the majority on every example except $x_i$, and would be correct every time. The majority will always predict 0 and will not incur any mistakes since there is in fact only one correct function. The only mistake will occur when $f_i$ predicts 0 with the majority on example $x_i$ when in fact it is correct. At that point the halving algorithm will eliminate all functions $f\in C$ which are not consistent, a set containing ONLY $f_i$, and learning ends after one mistake.
			\end{enumerate}
		\end{enumerate}
	
	\section{The Perceptron Algorithm and its Variants}
		\begin{enumerate}
			\item I used python3 to implement the Perceptron algorithm variants. I chose not to represent the data in a full 19-dimensional vector given how sparse the data is (imagine there were 100 dimensions). Instead I used an array of dictionaries where each entry in the array corresponds to a row item in the data, and each item is itself a dictionary whose key-value pairs correspond to the attributes and respective values. For examples, if the entry were to read: +1 3:15 4:29 13:30 .. the dictionary structure would appear: \{label: 1, 3:15, 4:29, 13:30\}. This implementation didn't allow me to leverage numpys vector library, but the additional overhead involved in looping was minimal and is more memory efficient. In terms of evaluating each of the variants, I chose to create a cross validation function to handle identifying the best hyper-parameters, one dedicated function for each variant, as well as one test function for each to evaluate the best parameter combinations on the test set.
			\item Given the most frequent label in the training set (+1), the accuracy on the development set is .56, and the test set is .525
			\item Noting that the random initialization may sometimes give different results with respect to the best hyper-parameters, in most cases I ran the 5-fold cross validation (for ten epochs each) several times and averaged them to see, on average, which parameters were \textit{actually} the best (however, the below results are based on a single random instance of the algorithm on the data). For ease of grading, I initialized the random seed to 1234 in order to have repeatable test runs. My results were as follows:\\ \\
			\textbf{Simple Perceptron}    \\
			a) rate = \{.1\}  \\
			b) cross-validation accuracy using rate=.1 was \textbf{0.61333}\\
			c) total number of updates on the training set was \textbf{6026}\\
			d) the best accuracy on the development set came with using 12 epochs and yielded an accuracy of \textbf{.765} (see below for the plot used to determine the best epoch to use). This yielded a weight vector, bias combination as follows:\\
			\begin{gather}
				[-5.268682  , -6.708682  , 19.291318  ,  0.761318  , -9.648682  ,\\
       -8.948682  , -3.218682  ,  1.871318  ,  0.12729463, -0.08059336,\\
        1.05738386,  0.16120544,  3.12386446,  2.01868857,  0.9683586 ,\\
        0.46600629, -3.01700207, -0.73008194, -2.128682  ] \\
				b = -3.6329519999999667
			\end{gather}
			e) using the weight vector and bias from epoch 12 to predict on the test set, the algorithm reported an accuracy of \textbf{.592040} \\
			f)
			\begin{center}
				\includegraphics[width=0.7\linewidth]{simple_plot}
			\end{center}
			~\\
			\textbf{Decaying Perceptron}    \\
			a) For the specific combination of the random shuffling of the training data and the initial weight vector and bias, I found the optimal initial rate = 1  \\
			b) cross-validation accuracy using rate=1 was \textbf{0.681333}\\
			c) total number of updates on the training set was \textbf{4314}\\
			d) the best accuracy on the development set came with using 6 epochs and yielded an accuracy of \textbf{.75} (see below for the plot used to determine the best epoch to use). This yielded a weight vector, bias combination as follows:\\
			\begin{gather}
			w = [ -6.10613189,  -7.66302416,  21.92743329,   0.49574096,\\
       -10.78001363,  -8.78491743,  -2.74323505,  -1.64237966,\\
         0.08385119,  -2.51841626,   4.09065277,   0.11088972,\\
         2.98168452,   1.99799225,   0.97096297,   0.47180217,\\
        -3.50512156,  -0.85424241,  -1.84955919] \\
			b = 3.007048000000001
			\end{gather}
			e) using the weight vector and bias from epoch 6 to predict on the test set, the algorithm reported an accuracy of \textbf{.736318} \\
			f)
			\begin{center}
				\includegraphics[width=0.7\linewidth]{decaying_plot}
			\end{center}
			~\\
			\textbf{Margin Perceptron}    \\
			a) For the specific combination of the random shuffling of the training data and the initial weight vector and bias, I found the optimal combination of hyper-parameters to be margin=.01, rate=.1  \\
			b) cross-validation accuracy using margin=.01, rate=1 was \textbf{0.681333}\\
			c) total number of updates on the training set was \textbf{4784}\\
			d) the best accuracy on the development set came with using 14 epochs and yielded an accuracy of \textbf{.74} (see below for the plot used to determine the best epoch to use). This yielded a weight vector, bias combination as follows:\\
			\begin{gather}
			w = [ -5.52290597,  -9.167429  ,  27.1623427 ,   2.71369141,\\
       -10.57019742, -10.36483082,  -6.25503214,  -6.43667752,\\
        -0.13033783,  -1.05421796,   2.44502523,   0.99051612,\\
         4.07930821,   2.18911149,   1.03575576,   0.50662002,\\
        -3.73971353,  -0.91322305,  -2.11006745] \\
			b = -12.992952000000002
			\end{gather}
			e) using the weight vector and bias from epoch 14 to predict on the test set, the algorithm reported an accuracy of \textbf{.706468} \\
			f)
			\begin{center}
				\includegraphics[width=0.7\linewidth]{margin_plot}
			\end{center}
			~\\
			\textbf{Averaged Perceptron}    \\
			a) For the specific combination of the random shuffling of the training data and the initial weight vector and bias, I found the optimal initialization of the hyper-parameter to be rate=.01  \\
			b) cross-validation accuracy using rate=.01 was \textbf{0.68}\\
			c) total number of updates on the training set was \textbf{5370}\\
			d) the best accuracy on the development set came with using 2 epochs and yielded an accuracy of \textbf{.79} (see below for the plot used to determine the best epoch to use). This yielded an averaged weight vector and bias combination as follows:\\
			\begin{gather}
			avg_w = [-1.09699957e+05, -1.62929829e+05,  3.91940721e+05, -1.86135988e+04,\\
       -2.11560441e+05, -1.49544642e+05, -4.14728821e+04,  1.53519572e+04,\\
        5.53541271e+03, -2.39957542e+04,  5.32795078e+03, -1.55674461e+02,\\
        4.51343788e+04,  2.02588990e+04,  7.68113024e+03,  3.23552673e+03,\\
       -7.23859805e+04, -1.77787760e+04, -1.47332186e+04] \\
			avg_b = 0.7600000000000005
			\end{gather}
			e) using the weight vector and bias from epoch 2 to predict on the test set, the algorithm reported an accuracy of \textbf{.761194} \\
			f)
			\begin{center}
				\includegraphics[width=0.7\linewidth]{average_plot}
			\end{center}
			~\\
			\textbf{Aggressive Perceptron}    \\
			a) For the specific combination of the random shuffling of the training data and the initial weight vector and bias, I found the optimal initialization of the hyper-parameter to be margin=.01  \\
			b) cross-validation accuracy using margin=.1 was \textbf{0.62267}\\
			c) total number of updates on the training set was \textbf{201}\\
			d) the best accuracy on the development set came with using 3 epochs and yielded an accuracy of \textbf{.78} (see below for the plot used to determine the best epoch to use). This yielded an averaged weight vector and bias combination as follows:\\
			\begin{gather}
			w = [ -7.6928314 , -13.59355951,  19.20728003,  -5.67217679,\\
       -12.60455981,  -3.55135434,   1.62398873,   0.56381289,\\
         0.3981029 ,   0.17702984,  -1.97917926,  -1.39918214,\\
         4.05054488,   2.79402866,   1.58171767,   0.8406619 ,\\
        -5.24727701,  -1.37110012,  -3.24841468]\\
			b = -0.07425752238948019
			\end{gather}
			e) using the weight vector and bias from epoch 3 to predict on the test set, the algorithm reported an accuracy of \textbf{.766169}\\
			f)
			\begin{center}
				\includegraphics[width=0.7\linewidth]{aggressive_plot}
			\end{center}
		\end{enumerate}
	
\end{document}